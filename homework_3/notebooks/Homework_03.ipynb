{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 03**  \n",
    "> In this homework, we will use MapReduce to calculate two canonical quantities in data analyses: the term frequency-inverse document frequency (tf-idf) measure, and the loss function for support vector machine.  \n",
    "> In this homework, you should write your own functions to calculate the quantities, instead of using functions from pyspark's MLLib library. Your code should utilize the RDDs and dataframes created from pyspark.  \n",
    "> While you probably will be able to use Chat to generate all of the necessary functions, I would encourage you to give it a try to design and process through how you may do it, before asking Chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TASK 01***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/25 02:26:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set up pyspark:\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# ignore warnings:\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# read in csv:\n",
    "agnews = spark.read.csv(\"data/agnews_clean.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix headers:\n",
    "agnews = agnews.withColumnRenamed('_c0', 'id')\n",
    "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map function for term-frequency computation:\n",
    "def tf_map(row):\n",
    "    \n",
    "    id = row['id']\n",
    "    terms = [term.lower() for term in row['filtered']]\n",
    "\n",
    "    length = len(terms)        # divide by length to get term frequency, not count!\n",
    "\n",
    "    for term in terms:\n",
    "        yield ((term, id), 1/length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# map/reduce term-frequency dataframe using spark\n",
    "tf_data = (agnews.rdd\n",
    "            .flatMap(tf_map)                      # tells spark we are applying over each row\n",
    "            .reduceByKey(lambda a, b: a+b)        # reduces by like keys\n",
    "            .collect()\n",
    "            )\n",
    "\n",
    "# returns ((term, doc), idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map function for inverse document frequency computation:\n",
    "def idf_map(row):\n",
    "    \n",
    "    id = row['id']\n",
    "    terms = [term.lower() for term in row['filtered']]\n",
    "    terms = set(terms)                            # want only unique terms\n",
    "\n",
    "    for term in terms:\n",
    "        yield (term, 1) \n",
    "\n",
    "# IDF = log(#docs / #docs_term)\n",
    "\n",
    "# (term, count) ==> yield per doc, sum count of each doc\n",
    "# --> sum count over each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# count number of documents:\n",
    "num_docs = agnews.count()\n",
    "\n",
    "# map/reduce inverse document frequency dataframe using spark\n",
    "idf_data = (agnews.rdd\n",
    "            .flatMap(idf_map)                     # tells spark we are applying over each row\n",
    "            .reduceByKey(lambda a, b: a+b)        # reduces by like keys (COUNT)\n",
    "            .mapValues(lambda x: np.log(num_docs / x)) # use map again to divide D by x and take log\n",
    "            .collect()\n",
    "            )\n",
    "\n",
    "# returns (term, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate tf-idf measure for each row in the agnews_clean.csv. Save the measures in a new column.\n",
    "    # tf_data --> ((term, doc), tf)\n",
    "    # idf_data --> (term, idf)\n",
    "\n",
    "# PARALLELIZE DATA: (too slow to compute linearly when attempted before)\n",
    "tf_rdd = sc.parallelize(tf_data)  # ((term, doc), tf)\n",
    "idf_rdd = sc.parallelize(idf_data)  # (term, idf)\n",
    "\n",
    "\n",
    "## JOIN DATASETS:\n",
    "# create new rdd for tf with key = term\n",
    "tf_by_term = tf_rdd.map(lambda x: (x[0][0], (x[0][1], x[1])))  # returns: (term, (doc_id, tf))\n",
    "\n",
    "# join with idf now that keys are same\n",
    "tfidf_rdd = tf_by_term.join(idf_rdd)                           # returns: (term, ((doc_id, tf), idf))\n",
    "\n",
    "# remap dataframe and calculate tf-idf\n",
    "tfidf_rdd = tfidf_rdd.map(lambda x: ((x[0], x[1][0][0]), x[1][0][1] * x[1][1]))  # returns: ((term, doc), tf-idf)\n",
    "\n",
    "\n",
    "## CREATE VECTORS OF TF-IDF:\n",
    "# group by doc\n",
    "tfidf_by_doc = tfidf_rdd.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
    "\n",
    "# create vectors of tf_idf values per document\n",
    "tfidf_vectors = tfidf_by_doc.groupByKey().mapValues(lambda keys: [tf_idf for term, tf_idf in keys]).sortBy(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD VECTORS AS A NEW COLUMN 'TF-IDF' TO THE DATAFRAME:\n",
    "\n",
    "# create clean dataframe with float values (error before):\n",
    "tfidf_clean = tfidf_vectors.map(lambda x: (x[0], [float(score) for score in x[1]]))\n",
    "\n",
    "# create dataframe of tf-idf values from vector and corresponding id:\n",
    "tfidf_df = tfidf_clean.map(lambda x: Row(id=x[0], tf_idf=x[1])).toDF()\n",
    "\n",
    "# join w/ old dataset (ON ID) into new dataset:\n",
    "agnews_joined = agnews.join(tfidf_df, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tf_idf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.1892216338539946, 0.4125512394225831, 0.5115985326511431, 0.563734318747707, 0.2953171727366614, 0.24754017186645658, 0.2773120373951269, 0.37743394553516213, 0.4572386180709258, 0.4468379768438066, 0.3372044607529448, 0.3643421454792778, 0.2584728642725166, 0.24678348986493034, 0.499114829314058, 0.2877107940095433]                                                                                                                                                                                                                          |\n",
      "|[0.16611784984952718, 0.35913687340984113, 0.19641046417882702, 0.24592768251620706, 0.24592768251620706, 0.23883727770874794, 0.48102210736229917, 0.19964935961395516, 0.28145443135455817, 0.2534665053879311, 0.2851373312446522, 0.33614365039815985, 0.1556269897557667, 0.1533541100403639, 0.37443433510272056, 0.2333223905577164, 0.35913687340984113, 0.3604747716127435, 0.4110465459521209, 0.3303379376753545, 0.20358864494328102, 0.1837934431328615]                                                                                      |\n",
      "|[0.16611784984952718, 0.15765250790140184, 0.417400887542718, 0.21497002416153688, 0.1227617002643825, 0.16817157068307878, 0.2274619996543202, 0.12872257976921794, 0.1680901354450561, 0.285864004330467, 0.17060209275512875, 0.3830867324749681, 0.14277532200924042, 0.08026707967043241, 0.35657859080014837, 0.28313029801014683, 0.2633135685435713, 0.4508852734249102, 0.28167569524618735, 0.255972118989524]                                                                                                                                   |\n",
      "|[0.17053284421704767, 0.1898997183872362, 0.324478643568105, 0.33274321954270536, 0.15969712503706046, 0.1650267812443044, 0.13394932212703356, 0.2057832028092643, 0.2578098186776328, 0.15043731768548949, 0.16022031730914288, 0.14507889141437585, 0.20949395177306526, 0.2284965552404658, 0.7168306746824437, 0.1973537176743789, 0.1890771769001148, 0.25188254045524316, 0.27861293130724324, 0.12468100563149095, 0.1698717076460444, 0.1751279339938823, 0.2581171817448437, 0.1929050573011279, 0.22418048797172685]                            |\n",
      "|[0.15994024564769707, 0.13552111644471848, 0.1131018693698805, 0.17676052180852383, 0.21601464260731984, 0.145856640464541, 0.15591601639460734, 0.13924134667488183, 0.10431117828830276, 0.16492932872045324, 0.22465153652572792, 0.15552292945064294, 0.27225416288029386, 0.0925193542091254, 0.35254772027561204, 0.13245598339561715, 0.19042797405490253, 0.18127557126337487, 0.2067185029184427, 0.1405921241478995, 0.09051819454875144, 0.10854419401585633, 0.12125053715366761, 0.27092999101551124, 0.12413070044238618, 0.3919911697751357]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# print out tf-idf values for first five documents:\n",
    "agnews_joined.select('tf_idf').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TASK 02***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set up pyspark:\n",
    "spark2 = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"SVM obj\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark2.sparkContext\n",
    "\n",
    "# ignore warnings:\n",
    "spark2.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# read in csv files:\n",
    "w = spark2.read.csv('data/w.csv', header=False, inferSchema=True)\n",
    "bias = spark2.read.csv('data/bias.csv', header=False, inferSchema=True)\n",
    "data_svm = spark2.read.csv('data/data_for_svm.csv', header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Design the MapReduce functions required to calculate the SVM loss function.\n",
    "\n",
    "# map loss function (summation):\n",
    "def map_loss(w, b, X_i, y_i):\n",
    "    \n",
    "    # calculate y_i(wT_x_i + b):\n",
    "    prod = y_i * (np.dot(w, X_i) + b) \n",
    "    \n",
    "    # choose max:\n",
    "    loss = max(0, 1-prod)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# reduce loss function (avg):\n",
    "def reduce_loss(losses):\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement a function loss_SVM(w, b, X, y) that computes the SVM objective for given w and b using the dataset stored in X and y.\n",
    "\n",
    "# w: weight vector\n",
    "# b: bias \n",
    "# X: features matrix\n",
    "# y: labels vector\n",
    "# lamb: lambda value\n",
    "\n",
    "def loss_SVM(w, b, X, y, lamb):\n",
    "\n",
    "    # compute regularization: --> L2 norm = sqrt(dot prod)\n",
    "    reg = lamb + np.dot(w, w) \n",
    "    \n",
    "    # map loss:\n",
    "    losses = [map_loss(X[i], y[i], w, b) for i in range(len(X))]\n",
    "\n",
    "    # reduce loss: \n",
    "    loss = reduce_loss(losses)\n",
    "\n",
    "    return loss + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. Use the dataset data_for_svm.csv, where:\n",
    "#    The first 64 columns contain the features (X)\n",
    "#    The last column contains the labels (y)\n",
    "\n",
    "# 4. Use the provided weights and bias (w.csv and bias.csv) to calculate the objective value:\n",
    "\n",
    "# convert variables to usable forms (np.arrays):\n",
    "w_array = np.array(w.rdd.first())\n",
    "bias_array = bias.rdd.first()[0]\n",
    "\n",
    "X_array = np.array([row[:-1] for row in data_svm.rdd.collect()])\n",
    "y_array = np.array([row[-1] for row in data_svm.rdd.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value is: 1.1032170403441435\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE AND REPORT OBJECTIVE VALUE:\n",
    "objective_value = loss_SVM(w_array, bias_array, X_array, y_array, 0.1)\n",
    "\n",
    "print(f\"Objective value is: {objective_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Design the MapReduce function required to make predictions using the SVM model:\n",
    "\n",
    "# prediction: if w * X_i + b >= 0 --> 1, else -1\n",
    "def map_pred(w, b, X_i):\n",
    "    \n",
    "    # calculate w * X_i + b:\n",
    "    score = np.dot(w, X_i) + b\n",
    "\n",
    "    # return corresponding prediction:\n",
    "    return 1 if score >= 0 else -1\n",
    "\n",
    "# # don't need reduce function- nothing to 'reduce'\n",
    "# def reduce_pred():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Predict on all of the data using the provided weights and bias:\n",
    "\n",
    "# create function to predict labels given w, b, X\n",
    "def predict(w, b, X):\n",
    "    return [map_pred(w, b, X_i) for X_i in X]\n",
    "\n",
    "# calculate predictions with provided inputs\n",
    "predictions = predict(w_array, bias_array, X_array)\n",
    "\n",
    "# # print predictions:\n",
    "# print(f\"predictions: {predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe4b0d4482ba336311d3990b0d9739c8b93c0bdb1c0f5a955322cac434aa27b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
