{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 03**  \n",
    "> In this homework, we will use MapReduce to calculate two canonical quantities in data analyses: the term frequency-inverse document frequency (tf-idf) measure, and the loss function for support vector machine.  \n",
    "> In this homework, you should write your own functions to calculate the quantities, instead of using functions from pyspark's MLLib library. Your code should utilize the RDDs and dataframes created from pyspark.  \n",
    "> While you probably will be able to use Chat to generate all of the necessary functions, I would encourage you to give it a try to design and process through how you may do it, before asking Chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once to download data- data should now be stored in work folder!\n",
    "# !curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/23 01:37:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set up pyspark:\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# ignore warnings:\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# read in csv:\n",
    "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix headers:\n",
    "agnews = agnews.withColumnRenamed('_c0', 'id')\n",
    "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each row contains the document id and a list of filtered words\n",
    "# agnews.show(5, truncate=30)\n",
    "\n",
    "agnews = agnews.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF MAP:\n",
    "def tf_map(row):\n",
    "    \n",
    "    id = row['id']\n",
    "    terms = [term.lower() for term in row['filtered']]\n",
    "\n",
    "    length = len(terms)        # divide by length to get term frequency, not count!\n",
    "\n",
    "    for term in terms:\n",
    "        yield ((term, id), 1/length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tf_data = (agnews.rdd\n",
    "            .flatMap(tf_map)                      # tells spark we are applying over each row\n",
    "            .reduceByKey(lambda a, b: a+b)        # reduces by like keys\n",
    "            .collect()\n",
    "            )\n",
    "\n",
    "# returns ((term, doc), idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF MAP:\n",
    "def idf_map(row):\n",
    "    \n",
    "    id = row['id']\n",
    "    terms = [term.lower() for term in row['filtered']]\n",
    "    terms = set(terms)\n",
    "\n",
    "    for term in terms:\n",
    "        yield (term, 1) \n",
    "\n",
    "# IDF = log(#docs / #docs_term)\n",
    "\n",
    "# (term, count) ==> yield per doc, sum count of each doc\n",
    "# --> sum count over each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = agnews.count()\n",
    "\n",
    "idf_data = (agnews.rdd\n",
    "            .flatMap(idf_map)                     # tells spark we are applying over each row\n",
    "            .reduceByKey(lambda a, b: a+b)        # reduces by like keys (COUNT AND DIVIDE BY NUM DOCS)\n",
    "            .mapValues(lambda x: np.log(num_docs / x))\n",
    "            .collect()\n",
    "            )\n",
    "\n",
    "# returns (term, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tf-idf measure for each row in the agnews_clean.csv. Save the measures in a new column.\n",
    "    # tf_data --> ((term, doc), tf)\n",
    "    # idf_data --> (term, idf)\n",
    "\n",
    "# PARALLELIZE DATA (too slow before):\n",
    "tf_rdd = sc.parallelize(tf_data)  # ((term, doc), tf)\n",
    "idf_rdd = sc.parallelize(idf_data)  # (term, idf)\n",
    "\n",
    "\n",
    "## JOIN DATASETS:\n",
    "# create new rdd for tf with key = term\n",
    "tf_by_term = tf_rdd.map(lambda x: (x[0][0], (x[0][1], x[1])))  # (term, (doc_id, tf))\n",
    "\n",
    "# join with idf now that keys are same\n",
    "tfidf_rdd = tf_by_term.join(idf_rdd)                           # (term, ((doc_id, tf), idf))\n",
    "\n",
    "# remap dataframe and calculate tf-idf\n",
    "tfidf_rdd = tfidf_rdd.map(lambda x: ((x[0], x[1][0][0]), x[1][0][1] * x[1][1]))  # ((term, doc), tf-idf)\n",
    "\n",
    "\n",
    "## CREATE VECTORS OF TF-IDF:\n",
    "# group by doc\n",
    "tfidf_by_doc = tfidf_rdd.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
    "\n",
    "# create vectors of tf_idf values per document\n",
    "tfidf_vectors = tfidf_by_doc.groupByKey().mapValues(lambda keys: [tf_idf for term, tf_idf in keys]).sortBy(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [0.06688737801810755,\n",
       "   0.08941321735745002,\n",
       "   0.17882643471490003,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.05675840264066563,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002,\n",
       "   0.08941321735745002]),\n",
       " (1,\n",
       "  [0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.05960881157163334,\n",
       "   0.03783893509377709,\n",
       "   0.05960881157163334,\n",
       "   0.08528092937014985,\n",
       "   0.04459158534540504,\n",
       "   0.1705618587402997,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.05960881157163334,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985,\n",
       "   0.08528092937014985]),\n",
       " (2,\n",
       "  [0.09594104554141858,\n",
       "   0.134119826036175,\n",
       "   0.09594104554141858,\n",
       "   0.09594104554141858,\n",
       "   0.04256880198049923,\n",
       "   0.03817878049475646,\n",
       "   0.0670599130180875,\n",
       "   0.09594104554141858,\n",
       "   0.09594104554141858,\n",
       "   0.05016553351358067,\n",
       "   0.09594104554141858,\n",
       "   0.09594104554141858,\n",
       "   0.09594104554141858,\n",
       "   0.09594104554141858,\n",
       "   0.09594104554141858,\n",
       "   0.05016553351358067,\n",
       "   0.09594104554141858,\n",
       "   0.10033106702716134,\n",
       "   0.05016553351358067,\n",
       "   0.09594104554141858,\n",
       "   0.09594104554141858]),\n",
       " (3,\n",
       "  [0.08223518189264449,\n",
       "   0.16447036378528898,\n",
       "   0.08223518189264449,\n",
       "   0.08223518189264449,\n",
       "   0.16447036378528898,\n",
       "   0.03648754455471362,\n",
       "   0.09817400698651661,\n",
       "   0.08223518189264449,\n",
       "   0.08223518189264449,\n",
       "   0.08223518189264449,\n",
       "   0.08223518189264449,\n",
       "   0.08223518189264449,\n",
       "   0.042999028725926286,\n",
       "   0.08223518189264449,\n",
       "   0.08223518189264449,\n",
       "   0.16447036378528898,\n",
       "   0.08223518189264449,\n",
       "   0.16447036378528898,\n",
       "   0.08223518189264449,\n",
       "   0.08223518189264449,\n",
       "   0.08223518189264449]),\n",
       " (4,\n",
       "  [0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.06108604879161034,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.08026485362172907,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.07675283643313487,\n",
       "   0.15350567286626973,\n",
       "   0.07675283643313487,\n",
       "   0.15350567286626973,\n",
       "   0.15350567286626973,\n",
       "   0.07675283643313487,\n",
       "   0.040132426810864534,\n",
       "   0.08026485362172907,\n",
       "   0.07675283643313487])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out tf-idf values for first five documents:\n",
    "tfidf_vectors.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'filtered', 'tfidf_vector']\n"
     ]
    }
   ],
   "source": [
    "# add each vector as new column in dataframe:\n",
    "\n",
    "# create dataframe from tf_idf vectors:\n",
    "tfidf_clean = tfidf_vectors.map(lambda x: (x[0], [float(score) for score in x[1]]))\n",
    "\n",
    "tfidf_df = tfidf_clean.map(\n",
    "    lambda x: Row(id=x[0], tfidf_vector=x[1])\n",
    ").toDF()\n",
    "\n",
    "# join w/ old dataset into new dataset:\n",
    "agnews_joined = agnews.join(tfidf_df, on='id', how='left')\n",
    "\n",
    "# display columns:\n",
    "print(agnews_joined.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe4b0d4482ba336311d3990b0d9739c8b93c0bdb1c0f5a955322cac434aa27b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
