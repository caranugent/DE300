{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW 03**  \n",
    "> In this homework, we will use MapReduce to calculate two canonical quantities in data analyses: the term frequency-inverse document frequency (tf-idf) measure, and the loss function for support vector machine.  \n",
    "> In this homework, you should write your own functions to calculate the quantities, instead of using functions from pyspark's MLLib library. Your code should utilize the RDDs and dataframes created from pyspark.  \n",
    "> While you probably will be able to use Chat to generate all of the necessary functions, I would encourage you to give it a try to design and process through how you may do it, before asking Chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tf-idf definition\n",
    "The tf-idf measure is defined as the following:\n",
    "\n",
    "Let $t$ be a term (a word), $d$ be a document, and $D$ be the collection of the documents.\n",
    "\n",
    "Term frequency (tf):\n",
    "\n",
    "$$\\mathrm{tf}(t, d) = \\frac{\\textrm{\\# occurrences of } t \\textrm{ in } d}{\\textrm{\\# terms in } d},$$\n",
    "\n",
    "Inverse document frequency (idf): $$\\mathrm{idf}(t, D) = \\log\\left(\\frac{\\textrm{\\# docs in } D}{\\textrm{\\# docs containing } t}\\right).$$\n",
    "\n",
    "As a result, the tf-idf measure is\n",
    "\n",
    "$$\\textrm{tf-idf}(t, d, D) = \\mathrm{tf}(t, d)\\times \\mathrm{idf}(t, D).$$\n",
    "\n",
    "Note: You can assume the number of documents in $D$ can be pre-computed, i.e. .count() in your dataframe/rdd.\n",
    "\n",
    "Tasks\n",
    "Design the MapReduce functions for calculating the tf-idf measure.\n",
    "Calculate tf-idf measure for each row in the agnews_clean.csv. Save the measures in a new column.\n",
    "Print out the tf-idf measure for the first 5 documents.\n",
    "Dataset\n",
    "The AG news dataset is cleaned and stored in agnews_clean.csv below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 33.2M  100 33.2M    0     0  5009k      0  0:00:06  0:00:06 --:--:-- 5010k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings:\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pyspark:\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "# read in csv:\n",
    "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix headers:\n",
    "agnews = agnews.withColumnRenamed('_c0', 'id')\n",
    "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|_c0|                      filtered|\n",
      "+---+------------------------------+\n",
      "|  0|[wall, st, bears, claw, bac...|\n",
      "|  1|[carlyle, looks, toward, co...|\n",
      "|  2|[oil, economy, cloud, stock...|\n",
      "|  3|[iraq, halts, oil, exports,...|\n",
      "|  4|[oil, prices, soar, time, r...|\n",
      "+---+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# each row contains the document id and a list of filtered words\n",
    "agnews.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. tf-idf definition**  \n",
    "    \n",
    "    The tf-idf measure is defined as the following:\n",
    "    \n",
    "    Let $t$ be a term (a word), $d$ be a document, and $D$ be the collection of the documents.\n",
    "    \n",
    "    Term frequency (tf):\n",
    "    \n",
    "    $$\\mathrm{tf}(t, d) = \\frac{\\textrm{\\# occurrences of } t \\textrm{ in } d}{\\textrm{\\# terms in } d},$$\n",
    "    \n",
    "    Inverse document frequency (idf): $$\\mathrm{idf}(t, D) = \\log\\left(\\frac{\\textrm{\\# docs in } D}{\\textrm{\\# docs containing } t}\\right).$$\n",
    "    \n",
    "    As a result, the tf-idf measure is\n",
    "    \n",
    "    $$\\textrm{tf-idf}(t, d, D) = \\mathrm{tf}(t, d)\\times \\mathrm{idf}(t, D).$$\n",
    "    \n",
    "    Note: You can assume the number of documents in $D$ can be pre-computed, i.e. .count() in your dataframe/rdd.\n",
    "    \n",
    "    Tasks\n",
    "    Design the MapReduce functions for calculating the tf-idf measure.\n",
    "    Calculate tf-idf measure for each row in the agnews_clean.csv. Save the measures in a new column.\n",
    "    Print out the tf-idf measure for the first 5 documents.\n",
    "    Dataset\n",
    "    The AG news dataset is cleaned and stored in agnews_clean.csv below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we need to calculate?\n",
    "\n",
    "* For each $d$, the counts of $t$,   \n",
    "> refer to word count example  \n",
    "* For each $d$, the counts of words,  \n",
    "* For each $t$, the counts of $d$ that contains $t$.  \n",
    "> what should be returned if we only want to know if the document contains $t$ of not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe4b0d4482ba336311d3990b0d9739c8b93c0bdb1c0f5a955322cac434aa27b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
