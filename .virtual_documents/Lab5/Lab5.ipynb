





# Required Package:
# psycopg2 2.9.10 (A PostgreSQL database adapter)
# pandas 2.0.3 (For data manipulation and analysis)
# sqlalchemy 2.0.37 (A SQL toolkit and Object Relational Mapper)
# pyarrow 14.0.1 (Provides support for efficient in-memory columnar data structures, part from Apache Arrow Objective)
import pandas as pd

#required for reading .xml files
import xml.etree.ElementTree as ET

#required for navigating machine's directory
import glob
import os.path

#required for communicating with SQL database
from sqlalchemy import create_engine





%pip install boto3


import boto3
import os

# my_aws_access_key_id="..."ASIAYAAO5HRMKWDLCJFP"
# my_aws_secret_access_key="..."MWyK0OX1F/P6VMJUAwIenfTdagBsfGNHz2oGh/ev"
# my_aws_session_token="..."IQoJb3JpZ2luX2VjEKb//////////wEaCXVzLWVhc3QtMiJHMEUCIANl9WP6uvB2izO6uOEqlQGqILGaCm7X9tAyRkJUuUqYAiEA1TjdPl59Y0HBR8OLJQb95o/DrqLUT3zoZ6JoTNmgSCUq6wIIUBAAGgw1NDk3ODcwOTAwMDgiDB7iboFeaXDcbvkEuCrIAuy+nhH9/nCjyxqof89nvc82blVMitBfh9UkUjM/GMy2+fTtHtTSadyukC/f476f7bftbihu5+dXTyyGNH5ixA6yFKVOQWYxlrs+LudwtUwGoc8MbnKSxkdWOo4hNgllfTZj/tf7hA8zV18yhYlFUihYiiWKbzrwki4J/D0YVrfEolok8ocgBe+mDkwHTn6iieXPNu5JEVGHvlce12x6Krm47BcBnceOfifG2xHdqlZFhnomhk6xAeUNTatWKOKMFHmovHUkUhuRaZJ8B80exGHzF3b2v2bjJ4uFr4x4W682cHkk6+6hGNBjbrsDNaFP1PGiHjcUGQZDdTG5vFP5H4ep3+U6WD/Vuf5km0Dgf+3ms7d2CFRdvr34KJpT1EfY5gjBk9wUvZsuvJkpvSEiub+v4HbTWNviGk2ZX/AFeMhTbRpmEC4K6bAw/5fqwAY6pwErmKz+zixyehFDdVwG7wjXFIWKmrefveTFZHWrV4EKkpy/tKV69zQnJ+1pktEwy2M0pVkOhGJQVlnRu4cy3CPWtZhiZBbvjhv6PynQUvGg0hy5MhiadJwVzaCzvqVatZNfrwLWJBKgxeUnZYxzz0BAPcHzAt2bIVJs8n767WLob54tOABY0H8BDpl6ZExX4+w1is7rO/1Lm64eStEnSYFrpJGPt3TuYQ=="

BUCKET_NAME = 'de300spring2025'   # Replace with your bucket name
S3_FOLDER = 'dinglin_xia/lab4_data/'             # The folder path in S3
LOCAL_DIR = './data/'      # Local directory to save files


def download_s3_folder(bucket_name, s3_folder, local_dir):
    """Download a folder from S3."""
    if not os.path.exists(local_dir):
        os.makedirs(local_dir)

    # List objects within the specified folder
    s3_resource = boto3.resource('s3',
                                aws_access_key_id=my_aws_access_key_id,
                                aws_secret_access_key=my_aws_secret_access_key,
                                aws_session_token=my_aws_session_token)
    bucket = s3_resource.Bucket(bucket_name)
    
    for obj in bucket.objects.filter(Prefix=s3_folder):
        # Define local file path
        local_file_path = os.path.join(local_dir, obj.key[len(s3_folder):])  
        
        if obj.key.endswith('/'):  # Skip folders
            continue
        
        # Create local directory if needed
        local_file_dir = os.path.dirname(local_file_path)
        if not os.path.exists(local_file_dir):
            os.makedirs(local_file_dir)
        
        # Download the file
        bucket.download_file(obj.key, local_file_path)
        print(f"Downloaded {obj.key} to {local_file_path}")


download_s3_folder(BUCKET_NAME, S3_FOLDER, LOCAL_DIR)





all_files = glob.glob('./data/*')

# Output the list of files
for file in all_files:
    print(file)





def extract_from_csv(file_to_process: str) -> pd.DataFrame:
    
    # add you line here to read the .csv file and return dataframe
    return pd.read_csv(file_to_process)





def extract_from_json(file_to_process: str) -> pd.DataFrame:
    
    # add you line here to read the .json file and return dataframe
    return pd.read_json(file_to_process, lines=True)





def extract_from_xml(file_to_process: str) -> pd.DataFrame:
    dataframe = pd.DataFrame(columns = columns)
    tree = ET.parse(file_to_process)
    root = tree.getroot()
    for person in root:
        car_model = person.find("car_model").text
        year_of_manufacture = int(person.find("year_of_manufacture").text)
        price = float(person.find("price").text)
        fuel = person.find("fuel").text
        sample = pd.DataFrame({"car_model":car_model, "year_of_manufacture":year_of_manufacture, "price":price, "fuel":fuel}, index = [0])
        dataframe = pd.concat([dataframe, sample], ignore_index=True)
    return dataframe





def extract() -> pd.DataFrame:
    extracted_data = pd.DataFrame(columns = columns)
    
    #for csv files
    for csv_file in glob.glob(os.path.join(folder, "*.csv")):
        extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)
    
    
    #add lines for json files
    for json_file in glob.glob(os.path.join(folder, "*.json")):
        extracted_data = pd.concat([extracted_data, extract_from_json(json_file)], ignore_index=True)
    
    #add lines for xml files
    for xml_file in glob.glob(os.path.join(folder, "*.xml")):
        extracted_data = pd.concat([extracted_data, extract_from_xml(xml_file)], ignore_index=True)
    
    return extracted_data





columns = ['car_model','year_of_manufacture','price', 'fuel']
folder = "data"
#table_name = "car_data"

# run
def main():
    data = extract()
    #insert_to_table(data, "car_data")
    
    return data

data = main()


data.head()





staging_file = "cars.parquet"
staging_data_dir = "staging_data"


def transform(df):
    print(f"Shape of data {df.shape}")

    # truncate price with 2 decimal place (add your code below)
    df['price'] = df['price'].apply(lambda x: int(x * 100)/100)

    # remove samples with same car_model (add your code below)
    df = df.drop_duplicates(subset=['car_model'])
    
    print(f"Shape of data {df.shape}")

    # Ensure the staging directory exists before writing the Parquet file
    if not os.path.exists(staging_data_dir):
        os.makedirs(staging_data_dir)
        print(f"Directory '{staging_data_dir}' created.")

    # write to parquet
    df.to_parquet(os.path.join(staging_data_dir, staging_file))
    return df


# print the head of your data
df = transform(data)
df.head()











# Database credentials
db_host = "localhost"
db_user = "can1469"
db_password = "@Tribal3d"
db_name = "local_db"

conn_string = f"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}"

engine = create_engine(conn_string)


# Test connection
df = pd.read_sql("SELECT * FROM pg_catalog.pg_tables;", con=engine)
print(df)


def insert_to_table(data: pd.DataFrame, conn_string:str, table_name:str):
    db = create_engine(conn_string) # creates a connection to the database using SQLAlchemy
    conn = db.connect() # Establishes a database connection
    data.to_sql(table_name, conn, if_exists="replace", index=False)
    conn.close()


# read from the .parquet file

def load() -> pd.DataFrame:
    data = pd.DataFrame()
    for parquet_file in glob.glob(os.path.join(staging_data_dir, "*.parquet")):
        data = pd.concat([pd.read_parquet(parquet_file),data])

    #insert_to_table(data, table_name)
    insert_to_table(data = data, conn_string = conn_string, table_name = 'ml_car_data')

    return data

data = load()
print(data.shape)
